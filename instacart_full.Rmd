---
title: "Instacart Market Basket Analysis"
author: "Thalia Taffe"
date: "`r Sys.Date()`"
output: pdf_document
---

## Introduction

This project focuses on analyzing "The Instacart Online Grocery Shopping Dataset 2017", which is a publicly available dataset provided by Instacart and hosted on Kaggle. Instacart is an American delivery company that offers grocery delivery and pick-up services in the United States and Canada. The dataset is anonymized and consists of a sample of over 3 million grocery orders from more than 200,000 Instacart users. The objective of this project is to gain insights into the shopping behavior of Instacart users.

For each user, the dataset provides between 4 and 100 of their orders, along with the sequence of products purchased in each order. Additionally, the week and hour of the day the order was placed, as well as a relative measure of time between orders, are also included in the dataset.

```{r}
library(dplyr)
library(ggplot2)
library(ggwordcloud)
library(tidytext)
library(topicmodels)
library(tidyr)
library(modelr)
library(ranger)
library(tidymodels)
library(rsample)
library(arules)
library(pROC)
```


```{r}
seedd = 542
theme_set(theme_bw())

aisles <- read.csv("aisles.csv")
departments <- read.csv("departments.csv")
order_products <- read.csv("order_products__train.csv")
orders <- read.csv("orders.csv")
products <- read.csv("products.csv")
```

The information from 5 separate csv files were used: aisles.csv, departments.csv, order_products_train.csv, orders.csv and products.csv. These files were linked by their unique ids using the appropriate key variables.

The features in each of these files are:

1. orders (3.4m rows, 206k users):

order_id: order identifier
user_id: customer identifier
eval_set: which evaluation set this order belongs in (see SET described below)
order_number: the order sequence number for this user (1 = first, n = nth)
order_dow: the day of the week the order was placed on
order_hour_of_day: the hour of the day the order was placed on
days_since_prior: days since the last order, capped at 30 (with NAs for order_number = 1)

2. products (50k rows):

product_id: product identifier
product_name: name of the product
aisle_id: foreign key
department_id: foreign key

3. aisles (134 rows):

aisle_id: aisle identifier
aisle: the name of the aisle

4. deptartments (21 rows):

department_id: department identifier
department: the name of the department

5. order_products__SET (30m+ rows):

order_id: foreign key
product_id: foreign key
add_to_cart_order: order in which each product was added to cart
reordered: 1 if this product has been ordered by this user in the past, 0 otherwise

where SET is one of the four following evaluation sets (eval_set in orders):

"prior": orders prior to that users most recent order (~3.2m orders)
"train": training data supplied to participants (~131k orders)
"test": test data reserved for machine learning competitions (~75k orders)

Only the orders with eval_set = "train" are selected for the analysis.


```{r, warning=FALSE}
orders <- orders %>% filter(eval_set == "train")

# merge aisles on products (aisle id) and departments on products (department id)
products <- inner_join(aisles, products, by = "aisle_id")
products <- inner_join(departments, products, by = "department_id")

# merge orders and order_products (on order id)
orders <- inner_join(orders, order_products, by = "order_id") %>%
  dplyr::select(!eval_set)

# merge both datasets ()
orders <- inner_join(orders, products, by = "product_id")
orders <- orders %>% 
  filter(!department == "missing" & days_since_prior_order > 0 & !department == "other")
```

The data is merged together into an "orders" data frame.

```{r}
# select random subset of 2000 id
set.seed(seedd)

ids <- unique(orders$user_id) %>% sample(500)
orders <- orders %>% filter(user_id %in% ids)

sum(unique(orders$order_id))
sum(unique(orders$user_id))
head(orders)
#write.csv(orders, file = "orders.csv", row.names = FALSE)
```

A random sample of 500 users id's are selected and their corresponding order details are considered.


```{r}
shopper_type <- function(df){
  convenience <- c("frozen", "bakery", "beverages", "babies", "pets", 
                 "snacks", "household", "personal care", "alcohol", "other")
  products <- df %>% group_by(user_id) %>% count()
  df$purpose <- "Unknown"
  c <- 0
  s <- 0
  
  x <- 1
  while (x <= length(df$department)){
    if (df$department[x] %in% convenience){
      c <- c + 1
      } else {
        s <- s + 1
    }
    if (c > s){
      df$purpose[x] <- "Convenience"
      } else {
        df$purpose[x] <- "Shopping"
      }
    x = x + 1
  }
  i <- 1
  while (i <= 500){
    if (products$n[i] <= 7){
      user <- products$user_id[i]
      df$purpose[df$user_id == user] <- "Convenience"
    }
    i <- i + 1
  }
  return(df)
}

orders <- shopper_type(orders)
```


```{r}
set.seed(seedd)
orders$purpose <- ifelse(orders$purpose == "Shopping", 1, 0)
orders <- orders %>% mutate(purpose = as.factor(purpose))

val_ids <- unique(orders$user_id) %>% sample(50)
val <- orders %>% filter(user_id %in% val_ids)
orders <- orders %>% filter(!(user_id %in% val_ids))
```


```{r}
## Checking for missing values

# count the number of missing values in each column
missing_counts <- colSums(is.na(orders))

# create a table with the missing value counts
missing_table <- data.frame(
  column_names = names(missing_counts),
  missing_count = missing_counts,
  row.names = NULL
)

missing_table
```

There seem to be no missing values in the dataset.


## EDA

## 1.The most commonly bought products in respective departments 
```{r}

# group by department and product
dept_prod_counts <- orders %>%
  group_by(department, product_name) %>%
  filter(reordered == 1) %>%
  summarise(prod_count = n()) %>%
  ungroup()

# select top 10 products for each department
top_prod <- dept_prod_counts %>%
  group_by(department) %>%
  filter(prod_count >= 7) 

top_prod <- top_prod %>%  arrange(desc(prod_count)) 
top_prod$department <- factor(top_prod$department)
top_prod %>% glimpse()

top_prod_produce <- top_prod %>% filter(department == 'produce')
top_prod_else <- top_prod %>% filter(department != 'produce')

top_prod_produce %>%
  ggplot(aes(x =prod_count, y = reorder(product_name,-prod_count),  fill = department)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ department, scales = "free",ncol = 2)+
  xlab("Product count")+
  ylab("Product name")

top_prod_else %>%
  ggplot(aes(x =prod_count, y = reorder(product_name,-prod_count), fill = department)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ department, scales = "free",ncol = 2) +
  xlab("Product count")+
  ylab("Product name")
```

To determine the popular products in each department, the data was grouped by department and product name. The analysis focused on the reordered products and the product count for each department was calculated. Products with a count greater than 7 were selected for analysis. To improve the clarity of the visualizations, the most reordered products from the 'produce' department were plotted separately from the products in other departments.

The plots revealed that the 'produce' department had the highest number of reordered products, which is expected as Instacart is primarily used for grocery shopping. Banana is the most commonly bought item from the department 'produce'. The 'dairy eggs' department also had a significant number of reordered products, with Organic Whole Milk being the most commonly purchased item, followed by Organic Unsweetened Almond Milk.

## 2. The most common aisles reordered from
```{r}
aisle_reorder <- orders %>%
                filter(reordered == 1) %>%
                group_by(department,aisle) %>% 
                summarise(num_reorders = n()) %>% 
                arrange(desc(num_reorders)) %>%
                ungroup() %>%
                glimpse()
                
        
aisle_reorder%>%
slice_max(num_reorders, n = 50) %>%
ggplot(aes(label = aisle, size = num_reorders, color = department))+
  geom_text_wordcloud() +
  scale_size_area(max_size = 10)
```

The wordcloud depicts the frequently reordered aisles, with each aisle name color-coded based on its department. The previous plot also showed that the aisles from the 'produce' department were the most commonly reordered, followed by those from the 'dairy eggs' department. The most commonly reordered aisles were identified as 'Fresh fruits', 'Fresh vegetables', 'Packaged vegetables', and 'Fruits'. In the 'dairy eggs' department, the most frequently reordered aisles were 'Yogurt', 'Packaged cheese', 'Milk', and 'Soy lactose-free'. 



## 3

```{r}
orders %>% 
  group_by(user_id) %>% 
  ggplot(aes(x = days_since_prior_order)) + geom_histogram() + xlab("Days Since Prior Order")
```


## 4
```{r}
orders %>% 
  #group_by(user_id) %>% 
  ggplot(aes(x = add_to_cart_order)) + geom_bar() + facet_wrap(~department)
```


#5
```{r}
#ggplot(orders, aes(x = purpose, groupby = user_id)) + geom_bar()
q <- orders %>% group_by(order_id) %>% count()
o <- orders %>% group_by(user_id) %>% count()  %>% rename(Products = n)
dim(o)[1]
dim(q)[1]
# same length = no repeat users

o %>% ggplot(aes(x = Products, fill = Products <= 7)) + geom_histogram() + xlab("Number of Products in Order") + ggtitle("Distribution of Number of Products Ordered") + scale_fill_manual(values = c("navy blue", "light blue"), labels = c("Shopping", "Convenience"))
orders %>% dplyr::count(purpose)
```

```{r}
# bag of words
order_sub <- orders %>% 
  select(department, product_name) #%>% 
  #mutate(department_id = as.integer(department_id))

(order_tidy <- order_sub %>% unnest_tokens(word, product_name))

order_counts <- order_tidy %>% 
  group_by(word, department) %>%
  summarize(term_frequency = n(), .groups = "drop") %>%
  arrange(desc(term_frequency))

order_counts
```

#6

```{r, warning = FALSE}
# all department wordcloud

idf_department <-order_counts %>% 
  group_by(department, word) %>%
  summarize(term_frequency = sum(term_frequency)) %>%
  bind_tf_idf(word, department, term_frequency) %>% 
  arrange(desc(tf_idf))

idf_department

idf_department %>% 
  slice_max(tf_idf, n = 8, with_ties = FALSE) %>%
  ggplot() + geom_text_wordcloud_area(aes(label = word, size = tf_idf)) +
  facet_wrap(~department) + scale_size_area(max_size = 4) 
```

## 7 Hourly order pattern
```{r}
orders %>% group_by(order_hour_of_day) %>% summarize(count = n()) %>% mutate(percentage = count/sum(count)) %>% ggplot(aes(x = as.factor(order_hour_of_day), y = percentage)) + geom_col() + labs(x = "Hour of day",y = "Percentage of Order", title = "Hourly Orders")
```

There seem to be a steady increase in the percentage of orders from 6 am to 11 am. The peak order hour seem to be the time before lunch maybe from 11 am to 12 pm. This could be for a quick lunch preparation. The percentage of orders in the afternoon hours seems to be relatively stable until 6 pm, after which it starts to decline gradually. The percentage of orders further decreases during the night hours.

# Topic Modeling
```{r}

products <- orders %>%
  ungroup() %>%
  select(order_id, product_name) 

# Create document-term matrix
prod_dtm <- products %>%
  count(order_id, product_name) %>%
  cast_tdm(order_id,product_name, n)

lda <- LDA(prod_dtm, k = 5, control = list(seed = 123))

#Word distribution associated with each topic
lda_topics <- tidy(lda, matrix = "beta")
#head(lda_topics, n = 20)

#Most common word combinations
#lda_topics %>% slice_max(beta, n = 10)

#Most common words in the topics
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 8) %>% 
  mutate(topic = factor(topic))

topic_names <- c("Salads-I",
                 "Cooking",
                 "Salads-II",
                 "Smoothie",
                 "Breakfast Mix")

top_terms %>% 
  mutate(topic =  forcats::as_factor(topic_names[topic])) %>%
  ggplot(aes(x = beta, y = term, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic,scale = "free_y",ncol = 2) +
  xlab("Relative Frequency")
```

The topic modeling analysis using LDA was conducted to gain insights into the shopping behavior of Instacart users. After generating five distinct topics, some topics were found to have similar product names. For instance, two topics included ingredients commonly used in salads, and were named Salads-I and Salads-II. The 'Cooking' topic mainly comprised vegetables that are typically used in cooking dishes, whereas the 'Smoothie' topic had products like whole milk, yogurt, and fruits such as strawberries and avocado, which are commonly used in making smoothies. The 'Breakfast Mix' topic consisted of items such as pure Irish butter and organic unsweetened almond milk that are typically used in breakfast dishes.

The findings from the topic modeling analysis suggest that users most frequently use Instacart to purchase items for making salads, smoothies, cooking, or for preparing breakfasts.




# Market Basket Analysis  
```{r}
#Convert the data to a transaction format
transactions <- orders %>%
 group_by(user_id,order_number) %>%
 summarise(products = list(as.character(product_name)))

##Performing market basket analysis using the Apriori algorithm
#rules <- apriori(transactions$products, parameter = list(support = 0.001, confidence = 0.8, minlen = 2,maxlen = 4))
#save(rules, file = "saved_rules.RDA")
load("saved_rules.RDA")

##Sorting
#sorted_rules <- sort(rules, by = "support", decreasing = TRUE)
#save(sorted_rules, file = "sorted_rules.RDA")
load("sorted_rules.RDA")

##Removing Redundant rules
#assoc_rules <- sorted_rules[1:5000]
#subset.rules <- which(colSums(is.subset(subset_rules, subset_rules)) > 1) 
#length(subset.rules)  
#subset.association.rules. <- assoc_rules[-subset.rules]


# Print the top 10 rules
#top10 <- inspect(head(subset.association.rules., n = 10)) 
#save(top10, file = "top10_rules.RDA")
load("top10_rules.RDA")
top10

```

Market Basket Analysis was performed using the Apriori algorithm to uncover associations between items purchased by customers, providing insights on frequently co-occurring items. The algorithm relies on frequent itemsets to generate association rules, operating on the premise that a subset of a frequent itemset must also be a frequent itemset. A frequent itemset is defined as an itemset whose support value exceeds a threshold value (support).

The analysis involves the computation of three key ratios: support, confidence, and lift. Support represents the fraction of occurrences of an itemset in the dataset, while confidence measures the probability that a rule will hold true for a new transaction with items on the left. Finally, lift is the ratio by which the confidence of a rule exceeds the expected confidence.

For this analysis, the minimum support is set at 0.001 and the minimum confidence is set at 0.8. The maximum and minimum lengths of item sets are set to 4 and 2, respectively, and rules are mined based on these settings. The resulting rules are sorted in descending order of support, ensuring that the rules reflect a sufficient occurrence of item set combinations in the dataset. To avoid redundancy, repeated rules are removed, and the top 10 rules are presented.

For instance, one of the rules states that there is a high likelihood of a customer purchasing Organic Red Bell Pepper and Organic Zucchini also buying Organic Yellow Onion.


# Train - Test Split
````{r}
set.seed(seedd)

split <- initial_split(orders, prop = 0.8, strata = purpose)
train <- training(split)
test <- testing(split)
```


## Modeling

Five predictive models were trained on the training data to classify customers as "Shopping" or "Convenience" based on their purchasing behavior. The response variable used was "purpose", while the predictor variables selected were "order_dow", "order_hour_of_day", "days_since_prior_order", "add_to_cart_order", "reordered", and "department".

The five models used are:

1. Logistic Regression, link = 'logit'

2. Logistic Regression, link = 'probit'

3. Lassso Regression

4. Random Forest Model

5. Optimized Random Forest Model


## 1. Logit
```{r}
formula <- purpose ~ order_dow + order_hour_of_day + days_since_prior_order + add_to_cart_order + reordered + department

fit_logit <- glm(formula = formula, data = train, family = binomial(link = "logit") )

summary(fit_logit)
```

Most of the predictor variables seem to have significant effect on the response variable 'purpose'.

```{r}
prediction_logit <- fit_logit %>% predict(test, type = "response")
```

The fitted model is then used to predict the test data.

## 2. Probit

```{r}
fit_probit <- glm(formula = formula, data = train, family = binomial(link = "probit") )
summary(fit_probit)

```

```{r}
prediction_probit <- fit_probit %>% predict(test, type = "response")
```

Most of the predictor variables have significant effect on the response variable 'purpose' and the fitted model is used to predict the test data. 

## 3. LASSO
```{r}
library(glmnet)
library(modelr)

y <- train %>% select(purpose) %>% as.matrix
  
order_x <- train %>%
  modelr::model_matrix(purpose ~ 
                          order_dow +
                          order_hour_of_day +
                          days_since_prior_order + 
                          add_to_cart_order +
                          reordered + 
                          department) 
#order_x %>% glimpse()
```


```{r}
fit_lasso <- glmnet(x = order_x,
                    y = y, 
                    family = "binomial")

coef(fit_lasso, s = 0.01)
```

Lasso regression was used for regularization. The variables selected are 'order_dow' ,'order_hour_of_day', 'days_since_prior_order', 'add_to_cart_order', 'reordered', 'department'.

```{r}
lasso_test <- test %>%
  modelr::model_matrix(purpose ~ 
                         order_dow +
                         order_hour_of_day +
                         days_since_prior_order +
                         add_to_cart_order +
                         reordered +
                         department ) 
lasso_test_matrix <- as.matrix(lasso_test)
prediction_lasso <-  predict(fit_lasso, newx = lasso_test_matrix, s = 0.01)
```



## 4. Random Forest

```{r}
set.seed(seedd)

default_rf <- rand_forest() %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

drf_fit <- default_rf %>% fit(purpose ~ ., data = train) %>% extract_fit_engine()
drf_fit

drf_pred <- predict(drf_fit, test, type = "response")
```


## 5. Optimized Random Forest

```{r}
set.seed(seedd)

trees <- seq(100, 1000, 100)
pred <- c(2, 3)

best_rf_error <- 1
error_list <- list()

for (t in trees){
  for (p in pred){
    rf_model <- rand_forest(mtry = p, trees = t) %>% 
      set_mode("classification") %>% 
      set_engine("ranger")
    rf_wrkflow_fit <- workflow() %>% 
      add_model(rf_model) %>%
      add_formula(purpose ~ .) %>% fit(train)
    err <- rf_wrkflow_fit$fit$fit$fit$prediction.error
    error_list <- append(error_list, err)
    if (err < best_rf_error){
      best_rf_error = err
      best_model = rf_wrkflow_fit
    }
  }
}
best_model
#error_list

best_rf_pred_class <- best_model %>% predict(test, type = "class")
best_rf_pred_prob <- best_model %>% predict(test, type = "prob")
best_rf_pred <- bind_cols(best_rf_pred_class, best_rf_pred_prob)
```


# Model Selection - ROC

The ROC curve and AUC is used to select the best model. 

```{r}
roc_logit = roc(test$purpose ~ prediction_logit, plot = TRUE, print.auc = TRUE,main ="ROC Curve - Logit ")

roc_probit = roc(test$purpose ~ prediction_probit, plot = TRUE, print.auc = TRUE, main ="ROC Curve - Probit ")

roc_lasso = roc(test$purpose ~ prediction_lasso, plot = TRUE, print.auc = TRUE, main ="ROC Curve - Lasso")

roc_rand_forest <- roc(test$purpose ~ drf_pred$predictions[,2], plot = TRUE, print.auc = TRUE, main = "ROC Curve - Random Forest")

roc_optim_rand_forest <- roc(test$purpose ~ best_rf_pred$.pred_1, plot = TRUE, print.auc = TRUE, main = "ROC Curve - Optimized Random Forest")
```

Upon comparing the AUC scores of different models, it was observed that the random forest and optimized random forest models had the highest AUC score of 0.975. Further analysis of the Out-of-Bag (OOB) errors revealed that the optimized random forest model had a lower OOB error of 0.0705, making it the preferred choice among the two models.

# Model Assessment

# 1.
```{r}
table(test$purpose, best_rf_pred$.pred_class)
```


# 2.

```{r}
outcome <- ifelse(test$purpose == best_rf_pred$.pred_class, TRUE, FALSE) %>% 
  as.factor()
ggplot(data = NULL, aes(test$purpose, best_rf_pred$.pred_1)) + 
  geom_point(aes(color = outcome)) + 
  scale_colour_manual(values = c("red", "black")) +
  xlab("Actual Class") + ylab("Predicted Probability of Class 1") +
  ggtitle("Class 1 Prediction Plot")
```


# 3.

```{r}

```


# Uncertainty Quantification

```{r}
# Test on validation set
val_pred <- predict(best_model, val)$.pred_class
table(val_pred, val$purpose)/576*100
```

```{r}
set.seed(seedd)
# auc on bootstrapped validation set

orders_bootstrap <- val %>% bootstraps(times = 10)
#control_orders <- control_resamples(extract = tidy)
bootstrap_fit_orders <- fit_resamples(best_model, orders_bootstrap)
for (samp in bootstrap_fit_orders$.metrics){
  s <- filter(samp, .metric == "roc_auc")
  print(s$.estimate)
}
```

## Add ons used

The five add ons demonstrated in this project are:

1. Writing a relevant custom function.

2. Performing a relevant word cloud graphic.

3. Fitting a relevant topic model.

4. Making a github repository for the project.

5. Performing Market Basket Analysis using Apriori algorithm.


## Source of the dataset

https://www.kaggle.com/competitions/instacart-market-basket-analysis/data





# Future steps / further refinement, boosting to fix misclassification of 0 (Convenience) class.